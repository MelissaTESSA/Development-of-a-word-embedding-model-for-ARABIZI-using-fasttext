{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Preprocessing for ARABIZI",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO14pObmM4wClHGLaxChn4R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelissaTESSA/Development-of-a-word-embedding-model-for-ARABIZI-using-fasttext/blob/main/Code_Preprocessing_for_ARABIZI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7-BmlXwUMLo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnKAkPE6P5pV"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOBeBblwXFOO"
      },
      "source": [
        "pip install XlsxWriter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnNz-pNykHYr"
      },
      "source": [
        "pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCogw9hAYivV"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMp5P2_woUhK"
      },
      "source": [
        "pip install PyArabic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wCDOay0v-Tg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import csv\n",
        "import unicodedata as ud #new added\n",
        "\n",
        "import sklearn\n",
        "import pickle\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras import utils\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.metrics import Accuracy,Recall,Precision\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSmsvvHjxZVY"
      },
      "source": [
        "#new added\n",
        "import pyarabic.araby as araby\n",
        "import pyarabic.number as number\n",
        "from pyarabic.araby import strip_harakat\n",
        "from pyarabic.araby import strip_tashkeel\n",
        "from pyarabic.araby import strip_lastharaka\n",
        "from pyarabic.araby import strip_tatweel\n",
        "from pyarabic.araby import strip_shadda\n",
        "\n",
        "import xlsxwriter\n",
        "import unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VSav6dOv-Qu"
      },
      "source": [
        "UP = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
        "LOW = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "LATIN = \"Ààéêèç\"\n",
        "SPACE = '\\t\\n\\r\\v\\f'\n",
        "fr_stopwords=stopwords.words('french')\n",
        "en_stopwords=stopwords.words('english')\n",
        "\n",
        "LETTERS =  'ہہچپڨابتةثجحخدذرزسشصضطظعغفقكلمنهويءآأؤإؤئئىىئ' #added characters\n",
        "\n",
        "#all_latin_chars = UP + LOW + LATIN\n",
        "\n",
        "punctuations = string.punctuation\n",
        "\n",
        "punctuations_list =  punctuations + SPACE\n",
        "\n",
        "french_punctuations_list= punctuations_list.replace(\"'\",\"\")\n",
        "\n",
        "#Full List of emogies to be removed\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "                       #u\"\\U0001F600-\\U0001F64F\"  # emoticons to keep \n",
        "                       u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                       u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                       u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                       u\"\\U00002702-\\U000027B0\"\n",
        "                       u\"\\U000024C2-\\U0001F251\"\n",
        "                       u\"\\U0001f932\"\n",
        "                       u\"\\u200f\"\n",
        "                       u\"\\U0001F914\"\n",
        "                       u\"\\U0001F923\"\n",
        "                       u\"\\u200D\"\n",
        "                       u\"\\u202c\"\n",
        "                       u\"\\u2069\"\n",
        "                       u\"\\u2066\"\n",
        "                       u\"\\U0001F926\"\n",
        "                       u\"\\U0001F917\"\n",
        "                       u\"\\U0001f928\"\n",
        "                       u\"\\t\"\n",
        "                       u\"\\u200e\"\n",
        "                       \"]+\", flags=re.UNICODE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvseyWwAv-Nm"
      },
      "source": [
        "#Remove small words \n",
        "def remove_minlen_word(text, threshold):\n",
        "    return \" \".join([w for w in text.split() if len(w) > threshold])\n",
        "\n",
        "#lower text\n",
        "def lower(text):\n",
        "    return text.lower()\n",
        "\n",
        "#Remove urls\n",
        "def remove_url(text):\n",
        "    text = re.sub('http[s]?://\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub('ftp?://\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub('www.\\S+', '', text, flags=re.MULTILINE)\n",
        "    return text\n",
        "\n",
        "\n",
        "#remove tags @ and hashtags #\n",
        "def remove_prefix(text, prefix):  # mainly used to remove hashtags and mensions\n",
        "    purged=[]\n",
        "    output=\"\"\n",
        "    for word in text.split():\n",
        "        if not word.startswith(prefix):\n",
        "            purged.append(word)\n",
        "    return \" \".join(purged)\n",
        "\n",
        "# Remove punctuation\n",
        "def remove_french_punctuations(text):\n",
        "    translator = str.maketrans('', '', french_punctuations_list)\n",
        "    return text.translate(translator)\n",
        "\n",
        "def remove_arabic_punct(text): #new added\n",
        "    return ''.join(c for c in text if not ud.category(c).startswith('P'))\n",
        "\n",
        "#remove french stop words, here we specified just Fr stop words, you can add other DZ word that you juge as stop words \n",
        "def remove_fr_stop_words(text):\n",
        "    temp_text=[]\n",
        "    for word in text.split(\" \"):\n",
        "        if word not in fr_stopwords:\n",
        "            temp_text.append(word)\n",
        "    return ' '.join(temp_text)\n",
        "\n",
        "# Remove repetting letters \n",
        "def remove_repeating_char(text):\n",
        "    result=[]\n",
        "    for word in text.split(\" \"):\n",
        "        temp=re.sub(r'(.)\\1+', r'\\1', word)\n",
        "        if len(temp)==1:\n",
        "             temp=temp+temp\n",
        "        result.append( temp)\n",
        "    return \" \".join(result)\n",
        "\n",
        "# remove emogies \n",
        "def remove_emoji(text):\n",
        "    return emoji_pattern.sub(r' ' , text)\n",
        "\n",
        "# remove extra stop words \n",
        "def remove_extra_white_spaces(text):\n",
        "    text= ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# remove digits delimited with white space\n",
        "def remove_digits(text):\n",
        "    text= re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
        "    return text\n",
        "\n",
        "# specify a list of generic words to be removed if you want =W we can add here the arabizi stop words \n",
        "def remove_words_generic(text,list_words=[]):\n",
        "    purged=[]\n",
        "    for word in text.split(\" \"):\n",
        "        if word not in list_words:\n",
        "            purged.append(word)\n",
        "    return \" \".join(purged)\n",
        "\n",
        "#remove all arabic characters from a text\n",
        "def remove_arabic_chars(text):\n",
        "    new_text = \"\"\n",
        "    for char in text:\n",
        "        if char not in LETTERS:\n",
        "            new_text += char\n",
        "    return new_text\n",
        "\n",
        "#normalize arabic numbers\n",
        "def normalize_arabic_numbers(text):\n",
        "    text = re.sub(\"٠\", \"0\", text)\n",
        "    text = re.sub(\"١\", \"1\", text)\n",
        "    text = re.sub(\"٢\", \"2\", text)\n",
        "    text = re.sub(\"٣\", \"3\", text)\n",
        "    text = re.sub(\"٤\", \"4\", text)\n",
        "    text = re.sub(\"٥\", \"5\", text)\n",
        "    text = re.sub(\"٦\", \"6\", text)\n",
        "    text = re.sub(\"٧\", \"7\", text)\n",
        "    text = re.sub(\"٨\", \"8\", text)\n",
        "    text = re.sub(\"٩\", \"9\", text)\n",
        "    return text\n",
        "\n",
        "# normalize arabizi characters\n",
        "def normalize_arabic(text):\n",
        "    text = re.sub(\"2\", \"a\", text)\n",
        "    text = re.sub(\"3\", \"aa\", text)\n",
        "    text = re.sub(\"7\", \"h\", text)\n",
        "    text = re.sub(\"9\", \"q\", text)\n",
        "    text = re.sub(\"8\",\"gh\", text)\n",
        "    text = re.sub(\"5\", \"kh\", text)\n",
        "    text = re.sub(\"6\", \"t\", text) #new added\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhoCg_Qka_au"
      },
      "source": [
        "#this is the pipeline of preprocessing \n",
        "def my_preproc(data=\"\",list_words=[]):\n",
        "    text = remove_url(data)\n",
        "    text= lower(text)\n",
        "    text= remove_minlen_word(text,1)  #words containing one letter\n",
        "    text=normalize_arabic_numbers(text) \n",
        "    text=remove_arabic_chars(text)\n",
        "    text=remove_digits(text) \n",
        "    text=normalize_arabic(text)\n",
        "    text=remove_prefix(text,\"@\")\n",
        "    text=remove_prefix(text,\"#\")\n",
        "    text=remove_words_generic(text, list_words)\n",
        "    text=remove_french_punctuations(text) \n",
        "    text=remove_arabic_punct(text) #new added\n",
        "    text=remove_fr_stop_words(text) \n",
        "    text=remove_repeating_char(text)\n",
        "    text=remove_extra_white_spaces(text) \n",
        "    text= remove_emoji(text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM4IBBSQfInJ"
      },
      "source": [
        "text=\"rani 7aba nro7 l alg8ia 187\"\n",
        "my_preproc(data=text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wQrpd-95tmg"
      },
      "source": [
        "#read csv file\n",
        "name =\"cevital\"\n",
        "df = pd.read_csv(name+\".csv\", error_bad_lines=False, engine = 'python')\n",
        "\n",
        "#compose list of stopwords and names from files\n",
        "with open('stopwords.txt','r') as f1:\n",
        "  with open('prenoms.txt','r') as f2:\n",
        "      l1 = f1.readlines()\n",
        "      l2 = f2.readlines()\n",
        "      lines = l1+l2\n",
        "      for i in range(len(lines)):\n",
        "        lines[i] = lines[i].split()[0].strip()\n",
        "        lines[i] = lines[i].lower()\n",
        "\n",
        "#preprocess the file\n",
        "for i in range(len(df)):\n",
        "  text = str(df.iloc[i,0]) \n",
        "  #new added : remove arabic characters and tashkeel\n",
        "  text = strip_tashkeel(text)\n",
        "  text = strip_harakat(text)\n",
        "  text = strip_lastharaka(text)\n",
        "  text = strip_shadda(text)\n",
        "  text = strip_tatweel(text)\n",
        "  #preprocess data\n",
        "  df.iloc[i,0] = my_preproc(data=text, list_words=lines)\n",
        "\n",
        "print(\"Original dataset :\", len(df))\n",
        "a = len(df)\n",
        "#remove duplicates\n",
        "df.drop_duplicates(inplace = True) \n",
        "print(\"New dataset :\", len(df))\n",
        "b= len(df)\n",
        "print(\"percentage of kept data :\", (b/a)*100)\n",
        "\n",
        "rows = []\n",
        "for i in range(len(df)):\n",
        "  #remove empty cells\n",
        "  if (str(df.iloc[i,0]).strip()!=\"\"):\n",
        "    rows.append(str(df.iloc[i,0]).strip())\n",
        "\n",
        "print(\"Length of final list :\",len(rows))\n",
        "\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook(name+'.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()\n",
        "\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UADFs7iaHjO"
      },
      "source": [
        "********************************************************************************\n",
        "Util functions\n",
        "********************************************************************************\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCVIEmBmCWG0"
      },
      "source": [
        "#detect language\n",
        "import fasttext\n",
        "import pandas as pd\n",
        "import xlsxwriter\n",
        "\n",
        "PRETRAINED_MODEL_PATH = './lid.176.bin'\n",
        "model = fasttext.load_model(PRETRAINED_MODEL_PATH)\n",
        "\n",
        "def predict_language(text):\n",
        "  predictions = model.predict([text])\n",
        "  return str(predictions[0])[12:14], predictions[1][0].item()\n",
        "\n",
        "df = pd.read_excel(r'last.xlsx')\n",
        "rows = []\n",
        "\n",
        "#for stats\n",
        "a = len(df)\n",
        "b=0\n",
        "for i in range(len(df)):\n",
        "    if predict_language(str(df.iloc[i,0]).strip())[1]<0.45 :\n",
        "        b+=1\n",
        "        rows.append(str(df.iloc[i,0]).strip())\n",
        "        print(str(df.iloc[i,0]).strip())\n",
        "\n",
        "print(\"***********************************************************\")\n",
        "print(a)\n",
        "print(b)\n",
        "print(\"percentage:\", (b/a)*100)\n",
        "\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook('last_n.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jJCTDn0CZjX"
      },
      "source": [
        "#remove arabic chars\n",
        "import pandas as pd\n",
        "import xlsxwriter\n",
        "import unicodedata\n",
        "\n",
        "def remove_accents(text):\n",
        "    return ''.join((c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')) \n",
        "\n",
        "chars= \"abcdefghijklmnopqrstuvwxyz\"\n",
        "def contains_text(text):\n",
        "  for c in text:\n",
        "    if c in chars:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "LETTERS =  'ییہہچپڨابتةثجحخدذرزسشصضطظعغفقكلمنهويءآأؤإؤئئىىئگـڤڤۦکی'\n",
        "#remove all arabic characters from a text\n",
        "def remove_arabic_chars(text):\n",
        "    new_text = \"\"\n",
        "    for char in text:\n",
        "        if char not in LETTERS:\n",
        "            new_text += char\n",
        "    return new_text\n",
        "\n",
        "df = pd.read_excel(r'01 dataset.xlsx')\n",
        "rows = []\n",
        "\n",
        "#for stats\n",
        "a = len(df)\n",
        "b=0\n",
        "for i in range(len(df)):\n",
        "    try:\n",
        "        #remove first word\n",
        "        text = str(df.iloc[i,0]).strip().split(\" \", 1)[1]\n",
        "        #remove accents (diacritics)\n",
        "        text = remove_accents(text)\n",
        "        #remove arabic letters\n",
        "        text = remove_arabic_chars(text)\n",
        "        #remove emojis and empty fields\n",
        "        if text!=\"\" and text.isnumeric()==False and contains_text(text):\n",
        "            b+=1\n",
        "            #rows.append(text.strip())\n",
        "            rows.append(str(df.iloc[i,0]).strip()) \n",
        "            print(text)\n",
        "    except:\n",
        "        print(\"exception\")\n",
        "        #pass\n",
        "    \n",
        "\n",
        "print(\"***********************************************************\")\n",
        "print(a)\n",
        "print(b)\n",
        "print(\"percentage:\", (b/a)*100)\n",
        "\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook('01 dataset.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hD8pBq1YCfC9"
      },
      "source": [
        "#remove diacritics\n",
        "import pandas as pd\n",
        "import xlsxwriter\n",
        "import re\n",
        "from unidecode import unidecode\n",
        "import unicodedata\n",
        "\n",
        "def remove_accents(text):\n",
        "    return ''.join((c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')) \n",
        "\n",
        "chars= \"abcdefghijklmnopqrstuvwxyz\"\n",
        "def contains_text(text):\n",
        "  for c in text:\n",
        "    if c in chars:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "LETTERS =  'ہہچپڨابتةثجحخدذرزسشصضطظعغفقكلمنهويءآأؤإؤئئىىئگـڤڤ'\n",
        "#remove all arabic characters from a text\n",
        "def remove_arabic_chars(text):\n",
        "    new_text = \"\"\n",
        "    for char in text:\n",
        "        if char not in LETTERS:\n",
        "            new_text += char\n",
        "    return new_text\n",
        "\n",
        "df = pd.read_excel(r'01 dataset.xlsx')\n",
        "rows = []\n",
        "\n",
        "#for stats\n",
        "a = len(df)\n",
        "b=0\n",
        "for i in range(len(df)):\n",
        "    try:\n",
        "        #text = unidecode.unidecode(str(df.iloc[i,0]).strip()) \n",
        "        text = remove_accents(str(df.iloc[i,0]).strip())\n",
        "        if text!=\"\" and text.isnumeric()==False and contains_text(text):\n",
        "            b+=1\n",
        "            rows.append(text.strip())\n",
        "            print(text)\n",
        "    except:\n",
        "        print(\"exception\")\n",
        "    \n",
        "\n",
        "print(\"***********************************************************\")\n",
        "print(a)\n",
        "print(b)\n",
        "print(\"percentage:\", (b/a)*100)\n",
        "\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook('01 dataset.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvhZf_lPCkXJ"
      },
      "source": [
        "#remove comments doesn't containing text (only emojis)\n",
        "import pandas as pd\n",
        "import xlsxwriter\n",
        "\n",
        "chars= \"abcdefghijklmnopqrstuvwxyz\"\n",
        "def contains_text(text):\n",
        "  for c in text:\n",
        "    if c in chars:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "df = pd.read_excel(r'Melissa_dataset_V02.xlsx')\n",
        "rows = []\n",
        "\n",
        "#for stats\n",
        "a = len(df)\n",
        "b=0\n",
        "for i in range(len(df)):\n",
        "    if contains_text(str(df.iloc[i,0]).strip()) :\n",
        "        b+=1\n",
        "        rows.append(str(df.iloc[i,0]).strip())\n",
        "        print(str(df.iloc[i,0]).strip())\n",
        "\n",
        "print(\"***********************************************************\")\n",
        "print(a)\n",
        "print(b)\n",
        "print(\"percentage:\", (b/a)*100)\n",
        "\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook('02 dataset.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ei24Z0mCxYg"
      },
      "source": [
        "#remove first word of each comment\n",
        "import pandas as pd\n",
        "import xlsxwriter\n",
        "\n",
        "chars= \"abcdefghijklmnopqrstuvwxyz\"\n",
        "def contains_text(text):\n",
        "  for c in text:\n",
        "    if c in chars:\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "df = pd.read_excel(r'02 dataset.xlsx')\n",
        "rows = []\n",
        "\n",
        "#for stats\n",
        "a = len(df)\n",
        "b=0\n",
        "for i in range(len(df)):\n",
        "    try:\n",
        "        text = str(df.iloc[i,0]).strip().split(\" \", 1)[1]\n",
        "        if text!=\"\" and text.isnumeric()==False and contains_text(text):\n",
        "            b+=1\n",
        "            rows.append(str(df.iloc[i,0]).strip())\n",
        "            print(text)\n",
        "    except:\n",
        "        print(\"exception\")\n",
        "    \n",
        "\n",
        "print(\"***********************************************************\")\n",
        "print(a)\n",
        "print(b)\n",
        "print(\"percentage:\", (b/a)*100)\n",
        "\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook('02 dataset.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDLmRvrxfQqS"
      },
      "source": [
        "'''\n",
        "import xlsxwriter\n",
        "import unidecode\n",
        "\n",
        "\n",
        "#read csv file\n",
        "df = pd.read_csv(\"bimo.csv\", error_bad_lines=False, engine = 'python')\n",
        "\n",
        "#compose list of stopwords and names\n",
        "with open('stopwords.txt','r') as f1:\n",
        "  with open('prenoms.txt','r') as f2:\n",
        "      l1 = f1.readlines()\n",
        "      l2 = f2.readlines()\n",
        "      lines = l1+l2\n",
        "      for i in range(len(lines)):\n",
        "        lines[i] = lines[i].split()[0].strip()\n",
        "        lines[i] = lines[i].lower()\n",
        "\n",
        "#preprocess the file\n",
        "for i in range(len(df)):\n",
        "  text = str(df.iloc[i,0])\n",
        "  df.iloc[i,0] = my_preproc(data=text, list_words=lines)\n",
        "  #df.iloc[i,0] = unidecode.unidecode(df.iloc[i,0])\n",
        "\n",
        "print(\"Original dataset :\", len(df))\n",
        "a = len(df)\n",
        "#remove duplicates\n",
        "df.drop_duplicates(inplace = True) \n",
        "print(\"New dataset :\", len(df))\n",
        "b= len(df)\n",
        "print(\"percentage of kept data :\", (b/a)*100)\n",
        "\n",
        "rows = []\n",
        "for i in range(len(df)):\n",
        "  #remove empty cells\n",
        "  if (str(df.iloc[i,0]).strip()!=\"\"):\n",
        "    rows.append(str(df.iloc[i,0]).strip())\n",
        "\n",
        "print(\"Length of final list :\",len(rows))\n",
        "\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook('db.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()\n",
        "\n",
        "print(\"Done\")'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo_mrcHkP5w0"
      },
      "source": [
        "'''\n",
        "#write into excel file\n",
        "workbook = xlsxwriter.Workbook('db.xlsx')\n",
        "worksheet = workbook.add_worksheet()\n",
        "worksheet.write_column('A1', rows)\n",
        "workbook.close()\n",
        "\n",
        "print(\"Done\")'''\n",
        "\n",
        "'''\n",
        "for i in range(len(df)):\n",
        "  if (str(df.iloc[i,0])!=\"\"):\n",
        "    print(df.iloc[i,0])\n",
        "\n",
        "with open('db.csv', 'w') as f:\n",
        "  # create the csv writer\n",
        "  writer = csv.writer(f)\n",
        "  for row in rows:\n",
        "    writer.writerow(row)\n",
        "'''\n",
        "\n",
        "\n",
        "#df.to_csv(\"db.csv\", index=False)\n",
        "#df.to_excel(\"db.xlsx\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGiIf_Ip_D1c"
      },
      "source": [
        "with open('stopwords.txt','r') as f1:\n",
        "  with open('prenoms.txt','r') as f2:\n",
        "      l1 = f1.readlines()\n",
        "      l2 = f2.readlines()\n",
        "      lines = l1+l2\n",
        "      for i in range(len(lines)):\n",
        "        lines[i] = lines[i].split()[0].strip()\n",
        "        lines[i] = lines[i].lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-chCJo3itEH"
      },
      "source": [
        "chars= \"abcdefghijklmnopqrstuvwxyz\"\n",
        "def contains_text(text):\n",
        "  for c in text:\n",
        "    if c in chars:\n",
        "      return True\n",
        "  return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmNd45aP5WT4"
      },
      "source": [
        "'''\n",
        "+araby.HARAKAT+araby.LIGUATURES+araby.TANWIN+araby.TASHKEEL+araby.SHORTHARAKAT+araby.HAMZAT+araby.ALEFAT\n",
        "+araby.WEAK+araby.YEHLIKE+araby.WAWLIKE+araby.TEHLIKE+araby.SMALL+araby.MOON+araby.SUN+araby.NAMES'''\n",
        "print(araby.is_arabicrange(\"ۦ\"))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}