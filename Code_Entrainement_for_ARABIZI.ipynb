{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Entrainement for ARABIZI",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNccYJUlFU7BnHZ70nGCP2J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MelissaTESSA/Development-of-a-word-embedding-model-for-ARABIZI-using-fasttext/blob/main/Code_Entrainement_for_ARABIZI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8afPOJ8g12EZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d1e6a950-40fe-40d2-adfc-bf273dec4fbf"
      },
      "source": [
        "'''\n",
        "#we trained the model on several parameters : \n",
        "  input             # training file path (required)\n",
        "    lr                # learning rate [0.1] 0.01-1\n",
        "    dim               # size of word vectors [100] 100-300\n",
        "    ws                # size of the context window [5]\n",
        "    epoch             # number of epochs [5] 5-30\n",
        "    minCount          # minimal number of word occurences [1]\n",
        "    minCountLabel     # minimal number of label occurences [1]\n",
        "    minn              # min length of char ngram [0] 2\n",
        "    maxn              # max length of char ngram [0] 5\n",
        "    neg               # number of negatives sampled [5]\n",
        "    wordNgrams        # max length of word ngram [1] 2-3\n",
        "    loss              # loss function {ns, hs, softmax, ova} [softmax]\n",
        "    bucket            # number of buckets [2000000]\n",
        "    thread            # number of threads [number of cpus]\n",
        "    lrUpdateRate      # change the rate of updates for the learning rate [100]\n",
        "    t                 # sampling threshold [0.0001]\n",
        "    label             # label prefix ['__label__']\n",
        "    verbose           # verbose [2]\n",
        "    pretrainedVectors # pretrained word vectors (.vec file) for supervised learning []\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n#we trained the model on several parameters : \\n  input             # training file path (required)\\n    lr                # learning rate [0.1] 0.01-1\\n    dim               # size of word vectors [100] 100-300\\n    ws                # size of the context window [5]\\n    epoch             # number of epochs [5] 5-30\\n    minCount          # minimal number of word occurences [1]\\n    minCountLabel     # minimal number of label occurences [1]\\n    minn              # min length of char ngram [0] 2\\n    maxn              # max length of char ngram [0] 5\\n    neg               # number of negatives sampled [5]\\n    wordNgrams        # max length of word ngram [1] 2-3\\n    loss              # loss function {ns, hs, softmax, ova} [softmax]\\n    bucket            # number of buckets [2000000]\\n    thread            # number of threads [number of cpus]\\n    lrUpdateRate      # change the rate of updates for the learning rate [100]\\n    t                 # sampling threshold [0.0001]\\n    label             # label prefix ['__label__']\\n    verbose           # verbose [2]\\n    pretrainedVectors # pretrained word vectors (.vec file) for supervised learning []\\n\""
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVgRuzK-DADu"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt7ZNof9GhJS"
      },
      "source": [
        "pip install fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjgILdPoH2_6"
      },
      "source": [
        "import pandas as pd\n",
        "import fasttext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m4QaApfzFz8"
      },
      "source": [
        "#some similarity metrics, the one we used is the cosine similarity function, it was observed and proven that it gives good results on semantical similarities between wr\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "from sklearn.metrics import jaccard_score\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "def sklearn_cosine(x,y):\n",
        "  x=x.reshape(1,-1)\n",
        "  y=y.reshape(1,-1)\n",
        "  return cosine_similarity(x, y)\n",
        "\n",
        "def euclidian(x,y):\n",
        "  x=x.reshape(1,-1)\n",
        "  y=y.reshape(1,-1)\n",
        "  return euclidean_distances(x, y)\n",
        "\n",
        "def cosine(x,y):\n",
        "  x=x.reshape(1,-1)\n",
        "  y=y.reshape(1,-1)\n",
        "  return cosine_distances(x, y)\n",
        "\n",
        "def scipy_cosine():\n",
        "    return 1. - cdist(x, y, 'cosine')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zNPRqANpUMB"
      },
      "source": [
        "#cosine similarity result\n",
        "def sim_cos(dico,n):\n",
        "  avg = 0\n",
        "  if n==0:\n",
        "    for k in dico.keys():\n",
        "      x=model.get_word_vector(k)\n",
        "      y=model.get_word_vector(dico[k])\n",
        "      result = sklearn_cosine(x,y)[0,0]\n",
        "      if result<0.61:\n",
        "        avg=avg+1\n",
        "  else:\n",
        "    for k in dico.keys():\n",
        "      x=model.get_word_vector(k)\n",
        "      y=model.get_word_vector(dico[k])\n",
        "      result = sklearn_cosine(x,y)[0,0]\n",
        "      if result>=0.61:\n",
        "        avg=avg+1\n",
        "  avg = avg/len(dico)\n",
        "  print(\"***************\")\n",
        "  print(\"Precision of the model on\",len(dico),\"examples using cosine_similarity metric :\",avg)\n",
        "\n",
        "#cosine distance\n",
        "def cos_dis(dico):\n",
        "  avg = 0\n",
        "  for k in dico.keys():\n",
        "    x=model.get_word_vector(k)\n",
        "    y=model.get_word_vector(dico[k])\n",
        "    result = cosine(x,y)[0,0]\n",
        "    avg=avg+result\n",
        "    print(k,\"---\",dico[k],\":\",result)\n",
        "  avg = avg/len(dico)\n",
        "  print(\"*********\\nAverge cosine dis result of\",len(dico),\"examples :\",avg)\n",
        "\n",
        "#euclidian distance\n",
        "def sim_euc(dico):\n",
        "  avg = 0\n",
        "  for k in dico.keys():\n",
        "    x=model.get_word_vector(k)\n",
        "    y=model.get_word_vector(dico[k])\n",
        "    result = euclidian(x,y)[0,0]\n",
        "    avg=avg+result\n",
        "    #print(k,\"---\",dico[k],\":\",result)\n",
        "  avg = avg/len(dico)\n",
        "  print(\"*********\\nAverge euclidian result of\",len(dico),\"examples :\",avg)\n",
        "\n",
        "#jaccard score\n",
        "def jacc_score(dico):\n",
        "  avg = 0\n",
        "  for k in dico.keys():\n",
        "    x=model.get_word_vector(k)\n",
        "    y=model.get_word_vector(dico[k])\n",
        "    result = jaccard_score(x, y, average='samples')\n",
        "    avg=avg+result\n",
        "    print(k,\"---\",dico[k],\":\",result)\n",
        "  avg = avg/len(dico)\n",
        "  print(\"*********\\nAverge jaccard score of\",len(dico),\"examples :\",avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecoMCWl7Y9r0"
      },
      "source": [
        "#similarity test function, takes in input a dictionary X and a label binary vector Y (Y is 0 when the corresponding couple isn't similar, 1 if there's similarity)\n",
        "def similarity(X,Y, threshold):\n",
        "  avg = 0\n",
        "  i = 0\n",
        "  for k in X.keys():\n",
        "    x=model.get_word_vector(k)\n",
        "    y=model.get_word_vector(X[k])\n",
        "    result = sklearn_cosine(x,y)[0,0]\n",
        "    if result<threshold:\n",
        "      v=0\n",
        "    else:\n",
        "      v=1\n",
        "    if v == Y[i]:\n",
        "      avg+=1\n",
        "    #print(k,\"---\",X[k],\":\",result,\"\\t\\t\\t\\t\",v,\"--\",Y[i])\n",
        "    i+=1\n",
        "  print(\"Number of true positives :\",avg, \"/\",len(X),\"examples\")\n",
        "  avg = avg/len(X)\n",
        "  print(\"Precision of the model on\",len(X),\"examples using cosine function :\",avg)\n",
        "  #return avg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FL7fxVNSCJxx"
      },
      "source": [
        "#model +\n",
        "#'skipgram',minn=1, maxn=4, wordNgrams=1, dim=30, epoch =2, lr = 0.18500000000000008 --> precision on the test set : 0.7613636363636364\n",
        "model = fasttext.train_unsupervised(\"/content/drive/Shareddrives/Arabizi project /dataset/Training_Dataset_ARABIZI.csv\", 'skipgram',minn=1, maxn=4, wordNgrams=1, dim=30, epoch =2, lr = 0.18500000000000008)\n",
        "model.save_model(\"/content/drive/Shareddrives/Arabizi project /dataset/final/model.bin\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TG2jk7xwckId"
      },
      "source": [
        "similarity(X,Y,0.61) #returns the precision on the test set 88 examples using cosine_similarity metric\n",
        "#sim_cos(dico,0)      #returns the precision on the test set 44 not similar examples using cosine_similarity metric\n",
        "#sim_cos(dico2,1)     #returns the precision on the test set 44 similar examples using cosine_similarity metric"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2002PfCgLE4"
      },
      "source": [
        "#Test set 2\n",
        "X = {\"dar\":\"maison\",\n",
        "      \"samet\":\"bassel\",\n",
        "      \"wled\":\"tfel\",\n",
        "      \"ghaba\":\"chjer\",\n",
        "      \"mra\":\"femme\",\n",
        "      \"rajel\":\"homme\",\n",
        "      \"ayetli\":\"kelemni\",\n",
        "      \"bnadem\":\"eabd\",\n",
        "      \"cahier\":\"ktab\",\n",
        "      \"tomobil\":\"karoussa\",\n",
        "      \"kebch\":\"aid\",\n",
        "      \"bligha\":\"sebat\",\n",
        "      \"zroudia\":\"batata\",\n",
        "      \"bhar\":\"yaoum\",\n",
        "      \"hayawan\":\"kebch\",\n",
        "      \"korsi\":\"yekeoud\",\n",
        "      \"hlib\":\"bakara\",\n",
        "      \"yema\":\"mama\",\n",
        "      \"nrouh\":\"nemchi\",\n",
        "      \"yheb\":\"yebghi\",\n",
        "      \"ykoul\":\"yehder\",\n",
        "      \"mliha\":\"hayla\",\n",
        "      \"yekra\":\"yekteb\",\n",
        "      \"ysegem\":\"yaedel\",\n",
        "      \"lyoum\":\"aujourd'hui\",\n",
        "      \"tbib\":\"mrid\",\n",
        "      \"marche\":\"tekdi\",\n",
        "      \"boucher\":\"lham\",\n",
        "      \"lham\":\"begri\",\n",
        "      \"regarde\":\"chouf\",\n",
        "      \"azul\":\"salam\",\n",
        "      \"tefla\":\"chira\",\n",
        "      \"saha\":\"choukran\",\n",
        "      \"aaref\":\"alem\",\n",
        "      \"kbir\":\"adjouz\",\n",
        "      \"chir\":\"tfel\",\n",
        "      \"yechti\":\"yebghi\",\n",
        "      \"yedjyah\":\"yeqbah\",\n",
        "      \"mrid\":\"sbitar\",\n",
        "      \"ydawi\":\"dwa\",\n",
        "      \"hadja\":\"aefsa\",\n",
        "      \"yghres\":\"chjer\",\n",
        "      \"kelb\":\"chien\",\n",
        "      \"nsksik\":\"question\",\n",
        "     #\n",
        "      \"bab\":\"nrouh\",\n",
        "      \"hchich\":\"hmar\",\n",
        "      \"chargeur\":\"dar\",\n",
        "      \"stylo\":\"rajel\",\n",
        "      \"zdjadj\":\"hchich\",\n",
        "      \"gaz\":\"nhel\",\n",
        "      \"farmadj\":\"cartable\",\n",
        "      \"chachra\":\"internet\",\n",
        "      \"banana\":\"ktab\",\n",
        "      \"ghali\":\"beid\",\n",
        "      \"saedni\":\"houma\",\n",
        "      \"ktab\":\"tefla\",\n",
        "      \"chejra\":\"mra\",\n",
        "      \"poto\":\"babour\",\n",
        "      \"hout\":\"zbel\",\n",
        "      \"taka\":\"bligha\",\n",
        "      \"chaba\":\"fletox\",\n",
        "      \"hram\":\"bacina\",\n",
        "      \"poupoun\":\"valiza\",\n",
        "      \"hadjra\":\"yaourt\",\n",
        "      \"carlaja\":\"scotch\",\n",
        "      \"rkhis\":\"mani\",\n",
        "      \"sma\":\"chargeur\",\n",
        "      \"hchicha\":\"mra\",\n",
        "      \"makla\":\"couronne\",\n",
        "      \"srir\":\"khedma\",\n",
        "      \"rassek\":\"newar\",\n",
        "      \"sport\":\"baba\",\n",
        "      \"chitan\":\"haraqa\",\n",
        "      \"tfel\":\"prise\",\n",
        "      \"taam\":\"bidoun\",\n",
        "      \"zbel\":\"drap\",\n",
        "      \"zaaim\":\"kavi\",\n",
        "      \"mous\":\"veste\",\n",
        "      \"ktabat\":\"saboun\",\n",
        "      \"radjel\":\"intelligent\",\n",
        "      \"triko\":\"ghaba\",\n",
        "      \"rqad\":\"maaen\",\n",
        "      \"fenyan\":\"telephone\",\n",
        "      \"djay\":\"khimar\",\n",
        "      \"conne\":\"tefla\",\n",
        "      \"bassel\":\"maaen\",\n",
        "      \"khfaf\":\"khimar\",\n",
        "      \"meftah\":\"calculatrice\",\n",
        "      }\n",
        "\n",
        "Y=[]\n",
        "for i in range(44):\n",
        "  Y.append(1)\n",
        "for i in range(45):\n",
        "  Y.append(0)\n",
        "#\n",
        "print(len(Y))\n",
        "print(len(X))\n",
        "#print(len(X_n))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMtBbSSrHzzE"
      },
      "source": [
        "#replaces the autotune-validation feature, it loops over all possible parameters (well defined range), calculate the precision for each model, and returns the best one with the \n",
        "#corresponding parameters\n",
        "def autotune():\n",
        "  #takes a training set in input\n",
        "  #train it and variate the parameters in a defined range for each\n",
        "  #returns at each training the score on a test set of 90 labaled examples\n",
        "  #we take the best score\n",
        "  best = 0\n",
        "  cpt = 0 #compteur de la boucle\n",
        "  for epoch in range(2,30,2):\n",
        "      for dim in range(5,50,5):\n",
        "        for wordngrams in range(1,6):\n",
        "          for minn in range(1,4):\n",
        "            for maxn in range(1,minn+4):\n",
        "              lr=0\n",
        "              for i in range(10):\n",
        "                cpt +=1 \n",
        "                print(\"--\",cpt,\"--\")\n",
        "                lr = lr + 0.02\n",
        "                model = fasttext.train_unsupervised(\"/content/drive/Shareddrives/Arabizi project /dataset/Training_Dataset_ARABIZI.csv\", 'skipgram',minn=minn, maxn=maxn, wordNgrams=wordngrams, dim=dim, epoch = epoch, lr = lr)\n",
        "                res = similarity(X,Y,0.6)\n",
        "                if res>best:\n",
        "                  best=res\n",
        "                  e=epoch\n",
        "                  d=dim\n",
        "                  wn=wordngrams\n",
        "                  mn=minn\n",
        "                  mx=maxn\n",
        "                  l=lr\n",
        "                  print(\"*********\\nAverge cosine result of\",len(X),\"examples :\",res)\n",
        "                  print(\"epoch:\",epoch,'dim:',dim,\"wngram:\",wordngrams,\"minn:\",minn,\"maxn:\",maxn,\"lr:\",lr)\n",
        "  print(\"\\nbest score is :\",best,\"at epoch:\",e,'dim:',d,\"wngram:\",wn,\"minn:\",mn,\"maxn:\",mx,\"lr:\",l)\n",
        "\n",
        "#execute\n",
        "autotune()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg23ss_tIaSk"
      },
      "source": [
        "#reduce dimensionality and plot results\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def tsne_plot(dict):\n",
        "    \"Creates and TSNE model and plots it\"\n",
        "    labels = []\n",
        "    tokens = []\n",
        "\n",
        "    for key,value in dict.items():\n",
        "        tokens.append(model.get_word_vector(key))\n",
        "        labels.append(key)\n",
        "\n",
        "        tokens.append(model.get_word_vector(value))\n",
        "        labels.append(value)\n",
        "    \n",
        "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=250, random_state=23)\n",
        "    new_values = tsne_model.fit_transform(tokens)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    for i in range(len(x)):\n",
        "        plt.scatter(x[i],y[i])\n",
        "        plt.annotate(labels[i],\n",
        "                     xy=(x[i], y[i]),\n",
        "                     xytext=(5, 2),\n",
        "                     textcoords='offset points',\n",
        "                     ha='right',\n",
        "                     va='bottom')\n",
        "        plt.xlim(-10,10)\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHjfCPAtLLzT"
      },
      "source": [
        "#plot test\n",
        "t_dico ={\"dar\":\"maison\",\n",
        "      \"samet\":\"bassel\",\n",
        "      \"wled\":\"tfel\",\n",
        "      \"ghaba\":\"chjer\",\n",
        "      \"mra\":\"femme\",\n",
        "      \"rajel\":\"homme\",\n",
        "      \"ayetli\":\"kelemni\",\n",
        "      \"bnadem\":\"eabd\",\n",
        "      \"cahier\":\"ktab\",\n",
        "      \"tomobil\":\"karoussa\",\n",
        "      \"kebch\":\"aid\",\n",
        "      \"bligha\":\"sebat\",}\n",
        "tsne_plot(t_dico)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jfgEM5v9Oy4"
      },
      "source": [
        "#Test Set\n",
        "#How to proceed : list of words that we seek for their nearest neighbors with our model\n",
        "dico ={\"bab\":\"nrouh\",\n",
        "      \"hchich\":\"hmar\",\n",
        "      \"chargeur\":\"dar\",\n",
        "      \"stylo\":\"rajel\",\n",
        "      \"zdjadj\":\"hchich\",\n",
        "      \"gaz\":\"nhel\",\n",
        "      \"farmadj\":\"cartable\",\n",
        "      \"chachra\":\"internet\",\n",
        "      \"banana\":\"ktab\",\n",
        "      \"ghali\":\"beid\",\n",
        "      \"saedni\":\"houma\",\n",
        "      \"ktab\":\"tefla\",\n",
        "      \"chejra\":\"mra\",\n",
        "      \"poto\":\"babour\",\n",
        "      \"hout\":\"zbel\",\n",
        "      \"taka\":\"bligha\",\n",
        "      \"chaba\":\"fletox\",\n",
        "      \"hram\":\"bacina\",\n",
        "      \"poupoun\":\"valiza\",\n",
        "      \"hadjra\":\"yaourt\",\n",
        "      \"carlaja\":\"scotch\",\n",
        "      \"rkhis\":\"mani\",\n",
        "      \"sma\":\"chargeur\",\n",
        "      \"hchicha\":\"mra\",\n",
        "      \"makla\":\"couronne\",\n",
        "      \"srir\":\"khedma\",\n",
        "      \"rassek\":\"newar\",\n",
        "      \"sport\":\"baba\",\n",
        "      \"chitan\":\"haraqa\",\n",
        "      \"tfel\":\"prise\",\n",
        "      \"taam\":\"bidoun\",\n",
        "      \"zbel\":\"drap\",\n",
        "      \"zaaim\":\"kavi\",\n",
        "      \"mous\":\"veste\",\n",
        "      \"ktabat\":\"saboun\",\n",
        "      \"radjel\":\"intelligent\",\n",
        "      \"triko\":\"ghaba\",\n",
        "      \"rqad\":\"maaen\",\n",
        "      \"fenyan\":\"telephone\",\n",
        "      \"djay\":\"khimar\",\n",
        "      \"conne\":\"tefla\",\n",
        "      \"bassel\":\"maaen\",\n",
        "      \"khfaf\":\"khimar\",\n",
        "      \"meftah\":\"calculatrice\",\n",
        "      }\n",
        "_dico ={\"dar\":\"maison\",\n",
        "      \"samet\":\"bassel\",\n",
        "       \"auto\":\"karoussa\",\n",
        "       \"yheb\":\"yebghi\",\n",
        "      \"ykoul\":\"yehder\",\n",
        "      \"mliha\":\"hayla\",\n",
        "      \"yekra\":\"yekteb\",\n",
        "      \"ydir\":\"ywasi\",\n",
        "      \"lyoum\":\"aujourd'hui\",\n",
        "      }\n",
        "dico2 ={\"dar\":\"maison\",\n",
        "      \"samet\":\"bassel\",\n",
        "      \"wled\":\"tfel\",\n",
        "      \"ghaba\":\"chjer\",\n",
        "      \"mra\":\"femme\",\n",
        "      \"rajel\":\"homme\",\n",
        "      \"ayetli\":\"kelemni\",\n",
        "      \"bnadem\":\"eabd\",\n",
        "      \"cahier\":\"ktab\",\n",
        "      \"tomobil\":\"karoussa\",\n",
        "      \"kebch\":\"aid\",\n",
        "      \"bligha\":\"sebat\",\n",
        "      \"zroudia\":\"batata\",\n",
        "      \"bhar\":\"yaoum\",\n",
        "      \"hayawan\":\"kebch\",\n",
        "      \"korsi\":\"yekeoud\",\n",
        "      \"hlib\":\"bakara\",\n",
        "      \"yema\":\"mama\",\n",
        "      \"nrouh\":\"nemchi\",\n",
        "      \"yheb\":\"yebghi\",\n",
        "      \"ykoul\":\"yehder\",\n",
        "      \"mliha\":\"hayla\",\n",
        "      \"yekra\":\"yekteb\",\n",
        "      \"ysegem\":\"yaedel\",\n",
        "      \"lyoum\":\"aujourd'hui\",\n",
        "      \"tbib\":\"mrid\",\n",
        "      \"marche\":\"tekdi\",\n",
        "      \"boucher\":\"lham\",\n",
        "      \"lham\":\"begri\",\n",
        "      \"regarde\":\"chouf\",\n",
        "      \"azul\":\"salam\",\n",
        "      \"tefla\":\"chira\",\n",
        "      \"saha\":\"choukran\",\n",
        "      \"aaref\":\"alem\",\n",
        "      \"kbir\":\"adjouz\",\n",
        "      \"chir\":\"tfel\",\n",
        "      \"yechti\":\"yebghi\",\n",
        "      \"yedjyah\":\"yeqbah\",\n",
        "      \"mrid\":\"sbitar\",\n",
        "      \"ydawi\":\"dwa\",\n",
        "      \"hadja\":\"aefsa\",\n",
        "      \"yghres\":\"chjer\",\n",
        "      \"kelb\":\"chien\",\n",
        "      \"nsksik\":\"question\",\n",
        "      }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Q1MJDaIiVE"
      },
      "source": [
        "try:\n",
        "  print(model.words)\n",
        "  print(len(model.words))\n",
        "except:\n",
        "  print('exception')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HR8_47FVyIn"
      },
      "source": [
        "**************************************************************************\n",
        "down here are some util functions\n",
        "**************************************************************************"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vz8wSvx7rn_A"
      },
      "source": [
        "#cosine smilarity function\n",
        "#import modules\n",
        "import numpy as np\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "def cosine(x,y):\n",
        "  return dot(x, y)/(norm(x)*norm(y))\n",
        "#define arrays\n",
        "x = np.array([1, 1, 1, 1, 0, 0, 0, 0, 0]) \n",
        "y = np.array([0, 0, 1, 1, 1, 1, 0, 0, 0])\n",
        "\n",
        "#calculate Cosine Similarity python\n",
        "result = cosine(x,y)\n",
        "\n",
        "print(\"The Cosine Similarity between two vectors is: \",result)\n",
        "\n",
        "#sklearn.metrics.pairwise.cosine_similarity(X, Y=None, dense_output=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Px1aMEj7PYGk"
      },
      "source": [
        "#print resulting vectors into a matrix\n",
        "import numpy as np\n",
        "words = model.words\n",
        "t = []\n",
        "for w in words[:5000]:\n",
        "  v = model.get_word_vector(w)\n",
        "  t.append(v)\n",
        "M = np.array(t)\n",
        "print(M.shape)\n",
        "print(M)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXuYcChM68qv"
      },
      "source": [
        "#split dataset into train, test, validation set\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_ratio = 0.7, validation_ratio = 0.15, test_ratio = 0.15\n",
        "dataX = pd.read_csv(\"/content/drive/Shareddrives/Arabizi project /TrainingFastText/Dataset_ARABIZI.csv\", engine=\"python\")\n",
        "x_train, x_test = sk.model_selection.train_test_split(dataX, test_size=1 - train_ratio)\n",
        "x_val, x_test = sk.model_selection.train_test_split(x_test, test_size=test_ratio/(test_ratio + validation_ratio)) \n",
        "print(len(x_train), len(x_val), len(x_test))\n",
        "x_train.to_csv('train.csv')\n",
        "x_val.to_csv('valid.csv')\n",
        "x_test.to_csv('test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}